{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4bf1528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FediSarray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK resources (comment out if already downloaded)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define activities list (replace with yours if needed)\n",
    "ACTIVITIES = [\n",
    "    \"Hiking in a nearby nature reserve\",\n",
    "    \"Painting or drawing outdoors\",\n",
    "    \"Cooking a new recipe from a different culture\",\n",
    "    \"Learning a new language through online courses\",\n",
    "    \"Playing a musical instrument or composing music\",\n",
    "    \"Yoga or meditation session to relax and rejuvenate\",\n",
    "    \"Volunteer at a local charity or community center\",\n",
    "    \"Gardening and planting flowers or vegetables\",\n",
    "    \"Writing short stories or poetry\",\n",
    "    \"Photography walk to capture interesting scenes or moments\",\n",
    "    \"Indoor or outdoor rock climbing\",\n",
    "    \"Joining a book club and discussing literature with others\",\n",
    "    \"Birdwatching in a nearby park or forest\",\n",
    "    \"DIY crafts such as knitting, woodworking, or pottery\",\n",
    "    \"Visiting a museum or art gallery for inspiration\",\n",
    "    \"Playing board games or hosting a game night with friends\",\n",
    "    \"Taking a dance class, whether it's salsa, tango, or hip-hop\",\n",
    "    \"Camping under the stars and telling stories around a campfire\",\n",
    "    \"Attending a local theater production or live music event\",\n",
    "    \"Exploring a new hobby like beekeeping or stargazing with a telescope\"\n",
    "]\n",
    "\n",
    "# Preprocess the activities\n",
    "def preprocess_activity(text):\n",
    "  # Lowercase text\n",
    "  text = text.lower()\n",
    "  # Remove stopwords\n",
    "  stop_words = stopwords.words('english')\n",
    "  text = [word for word in text.split() if word not in stop_words]\n",
    "  # Remove punctuation\n",
    "  text = [word for word in text if word.isalpha()]\n",
    "  return text\n",
    "\n",
    "# Create a dictionary to store categories and activities\n",
    "categories = defaultdict(list)\n",
    "\n",
    "# Preprocess each activity and categorize based on keywords\n",
    "for activity in ACTIVITIES:\n",
    "  preprocessed_activity = preprocess_activity(activity)\n",
    "  # Define keywords for each category (modify as needed)\n",
    "  if any(word in preprocessed_activity for word in ['hiking', 'nature', 'outdoors']):\n",
    "    categories['Outdoor'].append(activity)\n",
    "  elif any(word in preprocessed_activity for word in ['painting', 'drawing', 'art', 'creative']):\n",
    "    categories['Creative Arts'].append(activity)\n",
    "  elif any(word in preprocessed_activity for word in ['cooking', 'recipe', 'culture']):\n",
    "    categories['Culinary Arts'].append(activity)\n",
    "  elif 'language' in preprocessed_activity:\n",
    "    categories['Learning'].append(activity)\n",
    "  elif any(word in preprocessed_activity for word in ['music', 'instrument', 'compose']):\n",
    "    categories['Music'].append(activity)\n",
    "  elif any(word in preprocessed_activity for word in ['yoga', 'meditation', 'relax']):\n",
    "    categories['Relaxation'].append(activity)\n",
    "  elif 'volunteer' in preprocessed_activity:\n",
    "    categories['Social'].append(activity)\n",
    "  elif any(word in preprocessed_activity for word in ['gardening', 'planting']):\n",
    "    categories['Outdoor'].append(activity)  # Can be in 'Gardening' category if preferred\n",
    "  elif any(word in preprocessed_activity for word in ['writing', 'story', 'poetry']):\n",
    "    categories['Creative Arts'].append(activity)\n",
    "  elif 'photography' in preprocessed_activity:\n",
    "    categories['Creative Arts'].append(activity)\n",
    "  elif 'rock climbing' in preprocessed_activity:\n",
    "    categories['Sports'].append(activity)\n",
    "  elif 'book club' in preprocessed_activity:\n",
    "    categories['Social'].append(activity)\n",
    "  elif 'birdwatching' in preprocessed_activity:\n",
    "    categories['Outdoor'].append(activity)\n",
    "  elif 'craft' in preprocessed_activity:\n",
    "    categories['Creative Arts'].append(activity)\n",
    "  elif any(word in preprocessed_activity for word in ['museum', 'art gallery']):\n",
    "    categories['Cultural'].append(activity)\n",
    "  elif any(word in preprocessed_activity for word in ['board game', 'game night']):\n",
    "    categories['Social'].append(activity)\n",
    "  elif 'dance' in preprocessed_activity:\n",
    "    categories['Physical Activity'].append(activity)\n",
    "  elif any(word in preprocessed_activity for word in ['camping', 'campfire']):\n",
    "    categories['Outdoor'].append(activity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49e8ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "edaa8130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hiking', 'nearby', 'nature', 'reserve']\n",
      "['painting', 'drawing', 'outdoors']\n",
      "['cooking', 'new', 'recipe', 'different', 'culture']\n",
      "['learning', 'new', 'language', 'online', 'courses']\n",
      "['playing', 'musical', 'instrument', 'composing', 'music']\n",
      "['yoga', 'meditation', 'session', 'relax', 'rejuvenate']\n",
      "['volunteer', 'local', 'charity', 'community', 'center']\n",
      "['gardening', 'planting', 'flowers', 'vegetables']\n",
      "['writing', 'short', 'stories', 'poetry']\n",
      "['photography', 'walk', 'capture', 'interesting', 'scenes', 'moments']\n",
      "['indoor', 'outdoor', 'rock', 'climbing']\n",
      "['joining', 'book', 'club', 'discussing', 'literature', 'others']\n",
      "['birdwatching', 'nearby', 'park', 'forest']\n",
      "['diy', 'crafts', 'pottery']\n",
      "['visiting', 'museum', 'art', 'gallery', 'inspiration']\n",
      "['playing', 'board', 'games', 'hosting', 'game', 'night', 'friends']\n",
      "['taking', 'dance', 'whether']\n",
      "['camping', 'stars', 'telling', 'stories', 'around', 'campfire']\n",
      "['attending', 'local', 'theater', 'production', 'live', 'music', 'event']\n",
      "['exploring', 'new', 'hobby', 'like', 'beekeeping', 'stargazing', 'telescope']\n",
      "['Cooking a new recipe from a different culture', 'Learning a new language through online courses', 'Exploring a new hobby like beekeeping or stargazing with a telescope']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK resources (comment out if already downloaded)\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')  # Download tokenizer for sentence splitting\n",
    "\n",
    "# Define activities list (replace with yours if needed)\n",
    "ACTIVITIES = [\n",
    "    \"Hiking in a nearby nature reserve\",\n",
    "    \"Painting or drawing outdoors\",\n",
    "    \"Cooking a new recipe from a different culture\",\n",
    "    \"Learning a new language through online courses\",\n",
    "    \"Playing a musical instrument or composing music\",\n",
    "    \"Yoga or meditation session to relax and rejuvenate\",\n",
    "    \"Volunteer at a local charity or community center\",\n",
    "    \"Gardening and planting flowers or vegetables\",\n",
    "    \"Writing short stories or poetry\",\n",
    "    \"Photography walk to capture interesting scenes or moments\",\n",
    "    \"Indoor or outdoor rock climbing\",\n",
    "    \"Joining a book club and discussing literature with others\",\n",
    "    \"Birdwatching in a nearby park or forest\",\n",
    "    \"DIY crafts such as knitting, woodworking, or pottery\",\n",
    "    \"Visiting a museum or art gallery for inspiration\",\n",
    "    \"Playing board games or hosting a game night with friends\",\n",
    "    \"Taking a dance class, whether it's salsa, tango, or hip-hop\",\n",
    "    \"Camping under the stars and telling stories around a campfire\",\n",
    "    \"Attending a local theater production or live music event\",\n",
    "    \"Exploring a new hobby like beekeeping or stargazing with a telescope\"\n",
    "]\n",
    "\n",
    "# Preprocess the activities\n",
    "def preprocess_activity(text):\n",
    "  text = text.lower()\n",
    "  stop_words = stopwords.words('english')\n",
    "  text = [word for word in text.split() if word not in stop_words]\n",
    "  text = [word for word in text if word.isalpha()]\n",
    "  return text\n",
    "\n",
    "# Create a dictionary to store categories and activities\n",
    "categories = defaultdict(list)\n",
    "\n",
    "flat_list = [preprocess_activity(activity) for activity in ACTIVITIES]\n",
    "\n",
    "# Loop through each activity and features\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "  print(flat_list[i])\n",
    "  # Assign activity to categories based on important words\n",
    "  for word in flat_list[i]:\n",
    "    # Create new categories dynamically if they don't exist\n",
    "    if word not in categories:\n",
    "      categories[word] = []\n",
    "    categories[word].append(activity)\n",
    "\n",
    "print(categories[\"new\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a78295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "bad_words = [\"bad\", \"swear\"]\n",
    "bad_activities = [\"smoking\", \"procrastinate\"]\n",
    "\n",
    "text = \"I might smoke later\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "# Filter bad words\n",
    "filtered_tokens = [token for token in tokens if token not in bad_words]\n",
    "\n",
    "# Check for bad activities (simple matching)\n",
    "bad_activity_count = 0\n",
    "for i in range(len(tokens) - 1):  # Check bigrams (two consecutive words)\n",
    "  if tokens[i] + \" \" + tokens[i + 1] in bad_activities:\n",
    "    bad_activity_count += 1\n",
    "\n",
    "# Option 1: Remove bad words (replace with '-') and flag bad activities\n",
    "filtered_text = \" \".join(filtered_tokens).replace(\" \", \"-\")\n",
    "if bad_activity_count > 0:\n",
    "  filtered_text += \" (WARNING: Bad Activities)\"\n",
    "print(filtered_text)\n",
    "\n",
    "# Option 2: Flag text with details\n",
    "if bad_words or bad_activity_count:\n",
    "  print(\"Text contains issues:\")\n",
    "  if bad_words:\n",
    "    print(\"  - Bad words found\")\n",
    "  if bad_activity_count:\n",
    "    print(\"  - Bad activities detected:\", bad_activity_count)\n",
    "else:\n",
    "  print(\"Text seems clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e711b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be9c92eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'for', 'a', 'morning', 'run'], ['attend', 'a', 'work', 'meeting'], ['cook', 'dinner'], ['read', 'a', 'book'], ['watch', 'a', 'movie'], ['take', 'a', 'nap']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FediSarray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mdesired_num_categories)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(activities_features)\n\u001b[1;32m---> 34\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(activities_features)\n\u001b[0;32m     35\u001b[0m activity_categories \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Print activity-category mappings\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1471\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1471\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1472\u001b[0m         X,\n\u001b[0;32m   1473\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1474\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[0;32m   1475\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1476\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_x,\n\u001b[0;32m   1477\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1478\u001b[0m     )\n\u001b[0;32m   1480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1482\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample activities (replace with your actual data)\n",
    "activities = [\n",
    "    \"Go for a morning run\",\n",
    "    \"Attend a work meeting\",\n",
    "    \"Cook dinner\",\n",
    "    \"Read a book\",\n",
    "    \"Watch a movie\",\n",
    "    \"Take a nap\",\n",
    "]\n",
    "\n",
    "# Download required NLTK resources (do this once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Preprocess and tokenize activities\n",
    "activities_tokenized = [nltk.word_tokenize(activity.lower()) for activity in activities]\n",
    "\n",
    "# **Optional Feature Engineering:**\n",
    "# - Consider using POS tagging, NER, or TF-IDF for more robust features\n",
    "# - Example with POS tagging (uncomment if desired)\n",
    "#from nltk.tag import pos_tag\n",
    "#activities_pos_tagged = [pos_tag(sentence) for sentence in activities_tokenized]\n",
    "#activities_features = activities_pos_tagged  # Replace with your chosen features\n",
    "\n",
    "# Use the original tokenized sentences as features for simplicity\n",
    "activities_features = activities_tokenized\n",
    "\n",
    "# Create dynamic categories using K-Means clustering (adjust parameters as needed)\n",
    "desired_num_categories = 5  # Adjust based on your desired number of categories\n",
    "kmeans = KMeans(n_clusters=desired_num_categories)\n",
    "print(activities_features)\n",
    "kmeans.fit(activities_features)\n",
    "activity_categories = kmeans.labels_\n",
    "\n",
    "# Print activity-category mappings\n",
    "for i, activity in enumerate(activities):\n",
    "    print(f\"{activity} -> Category: {activity_categories[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6fdfb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FediSarray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'for', 'a', 'morning', 'run'], ['attend', 'a', 'work', 'meeting'], ['cook', 'dinner'], ['read', 'a', 'book'], ['watch', 'a', 'movie'], ['take', 'a', 'nap']]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m desired_num_categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Adjust based on your desired number of categories\u001b[39;00m\n\u001b[0;32m     28\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mdesired_num_categories)\n\u001b[1;32m---> 29\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(activities_features)\n\u001b[0;32m     30\u001b[0m activity_categories \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Print activity-category mappings\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1471\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1471\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1472\u001b[0m         X,\n\u001b[0;32m   1473\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1474\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[0;32m   1475\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1476\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_x,\n\u001b[0;32m   1477\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1478\u001b[0m     )\n\u001b[0;32m   1480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1482\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample activities (replace with your actual data)\n",
    "activities = [\n",
    "    \"Go for a morning run\",\n",
    "    \"Attend a work meeting\",\n",
    "    \"Cook dinner\",\n",
    "    \"Read a book\",\n",
    "    \"Watch a movie\",\n",
    "    \"Take a nap\",\n",
    "]\n",
    "\n",
    "# Download required NLTK resources (do this once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Preprocess and tokenize activities\n",
    "activities_tokenized = [nltk.word_tokenize(activity.lower()) for activity in activities]\n",
    "\n",
    "# Filter out empty sentences (if desired)\n",
    "activities_filtered = [sentence for sentence in activities_tokenized if sentence]\n",
    "activities_features = activities_filtered\n",
    "\n",
    "print(activities_features)\n",
    "\n",
    "# Create dynamic categories using K-Means clustering (adjust parameters as needed)\n",
    "desired_num_categories = 3  # Adjust based on your desired number of categories\n",
    "kmeans = KMeans(n_clusters=desired_num_categories)\n",
    "kmeans.fit(activities_features)\n",
    "activity_categories = kmeans.labels_\n",
    "\n",
    "# Print activity-category mappings\n",
    "for i, activity in enumerate(activities):\n",
    "    print(f\"{activity} -> Category: {activity_categories[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ea6a165",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Download required NLTK resources (do this once)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#nltk.download('punkt')\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Preprocess and tokenize activities\u001b[39;00m\n\u001b[0;32m     19\u001b[0m activities_tokenized \u001b[38;5;241m=\u001b[39m [nltk\u001b[38;5;241m.\u001b[39mword_tokenize(activity) \u001b[38;5;28;01mfor\u001b[39;00m activity \u001b[38;5;129;01min\u001b[39;00m activities]\n\u001b[1;32m---> 20\u001b[0m activities_tokenized \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(activities_tokenized)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(activities_tokenized)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# **Feature Engineering:**\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# - Use TF-IDF Vectorizer to convert text to numerical features\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample activities (replace with your actual data)\n",
    "activities = [\n",
    "    \"Go for a morning run\",\n",
    "    \"Attend a work meeting\",\n",
    "    \"Cook dinner\",\n",
    "    \"Read a book about machine learning\",\n",
    "    \"Watch a movie with friends\",\n",
    "    \"Take a nap after lunch\",\n",
    "]\n",
    "\n",
    "# Download required NLTK resources (do this once)\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Preprocess and tokenize activities\n",
    "activities_tokenized = [nltk.word_tokenize(activity) for activity in activities]\n",
    "activities_tokenized = np.array(activities_tokenized)\n",
    "print(activities_tokenized)\n",
    "\n",
    "# **Feature Engineering:**\n",
    "# - Use TF-IDF Vectorizer to convert text to numerical features\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
    "activities_features = vectorizer.fit_transform(activities_tokenized)\n",
    "\n",
    "# Create a KMeans model\n",
    "desired_num_categories = 3  # Adjust based on your desired number of clusters\n",
    "kmeans = KMeans(n_clusters=desired_num_categories)\n",
    "\n",
    "print(activities_features)\n",
    "# Fit the model to features\n",
    "kmeans.fit(activities_features)\n",
    "\n",
    "# Extract cluster labels\n",
    "activity_categories = kmeans.labels_\n",
    "\n",
    "# Print activity-category mappings\n",
    "for i, activity in enumerate(activities):\n",
    "    print(f\"{activity} -> Category: {activity_categories[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11088656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9d175f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK resources (comment out if already downloaded)\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')  # Download tokenizer for sentence splitting\n",
    "\n",
    "# Define activities list (replace with yours if needed)\n",
    "ACTIVITIES = [\n",
    "    \"Painting or drawing outdoors\",\n",
    "        \"running or drinking outdoor\",\n",
    "    \"Playing a musical instrument or composing music\",\n",
    "    \"Yoga or meditation session to relax and rejuvenat\",\n",
    "    \"Exploring a new hobby like beekeeping or stargazing with a telescope\",\n",
    "    \"playing a guitar\",\n",
    "    \"better health\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5c8ad06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paint', 'draw', 'outdoor']\n",
      "['run', 'drink', 'outdoor']\n",
      "['play', 'music', 'instrument', 'compos', 'music']\n",
      "['yoga', 'medit', 'session', 'relax', 'rejuvenat']\n",
      "['explor', 'new', 'hobbi', 'like', 'beekeep', 'stargaz', 'telescop']\n",
      "['play', 'guitar']\n",
      "['good', 'health']\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Preprocess the activities\n",
    "def preprocess_activity(text):\n",
    "  text = text.lower()\n",
    "  stop_words = stopwords.words('english')\n",
    "  text = [word for word in text.split() if word not in stop_words]\n",
    "  text = [word for word in text if word.isalpha()]\n",
    "  text = [lemmatizer.lemmatize(word, pos='v') for word in text]\n",
    "  text = [lemmatizer.lemmatize(word, pos='a') for word in text]\n",
    "  print(text)\n",
    "  return text\n",
    "\n",
    "# Create a dictionary to store categories and activities\n",
    "categories = defaultdict(list)\n",
    "\n",
    "activity_category_list = [preprocess_activity(activity) for activity in ACTIVITIES]\n",
    "\n",
    "# Loop through each activity and features\n",
    "for i, activity in enumerate(ACTIVITIES):\n",
    "  # Assign activity to categories based on important words\n",
    "  for word in activity_category_list[i]:\n",
    "    # Create new categories dynamically if they don't exist\n",
    "    if word not in categories:\n",
    "      categories[word] = []\n",
    "    categories[word].append(activity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "928a66ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : outdoors\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"outdoors\",pos=\"v\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
